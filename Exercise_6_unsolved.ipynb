{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6\n",
        "\n",
        "Same rules as previous exercises apply.\n",
        "\n",
        "**NOTE**: Submit the ipynb, not PDF for this exercise.\n",
        "\n",
        "Chose the GPU runtime with T4 GPU."
      ],
      "metadata": {
        "id": "0sW0qf6IPe9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "1nSYziLL8eB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4xrIb-pPb2L"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from whisper import load_model"
      ],
      "metadata": {
        "id": "cbqXN-DPx_Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the 'small' model\n",
        "model_baseline = ..."
      ],
      "metadata": {
        "id": "EFwURfemyBAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the number of filterbanks used\n",
        "model_baseline.dims.n_mels"
      ],
      "metadata": {
        "id": "5mhCNzsVC7Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_baseline = None  # free memory"
      ],
      "metadata": {
        "id": "BAmICs1kC9Ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complete the dataloader (5 pt)"
      ],
      "metadata": {
        "id": "ZxGEinnJyPH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a code to fine-tune with the ami train set and evaluate with the ami test set\n",
        "\n",
        "# define data loader given that the audio files are in ami_train_audio_segmented\n",
        "# and the transcriptions are in ami_train_audio_segmented/text. The text file is in the format\n",
        "# <audio_file_name> <transcription>\n",
        "\n",
        "import whisper.audio as whisper_audio\n",
        "import torch\n",
        "\n",
        "audio_files = []\n",
        "transcriptions = []\n",
        "import os\n",
        "# TODO: change paths in two places\n",
        "with open('/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/ex6_files/text') as f:\n",
        "    for line in f:\n",
        "        audio_file, *transcription = line.strip().split()\n",
        "        transcription = ' '.join(transcription)\n",
        "        audio_files.append(os.path.join('/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/ex6_files/', f'{audio_file}.wav'))\n",
        "        transcriptions.append(transcription)\n",
        "\n",
        "def extract_audio_features(audio_file):\n",
        "    # find the number of samples for the audio file\n",
        "    n_samples = ...\n",
        "    padding_length = whisper_audio.N_SAMPLES - n_samples\n",
        "    return whisper_audio.log_mel_spectrogram(audio_file, n_mels=..., padding=padding_length)\n",
        "\n",
        "# define data loader given that the audio files are in ami_test_audio_segmented\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_files, transcriptions):\n",
        "        self.audio_files = audio_files\n",
        "        self.transcriptions = transcriptions\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        mel = extract_audio_features(self.audio_files[idx])\n",
        "        return mel, self.transcriptions[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    mels, transcriptions = zip(*batch)\n",
        "    return torch.stack(mels), transcriptions\n",
        "\n",
        "train_dataset = AudioDataset(audio_files, transcriptions)"
      ],
      "metadata": {
        "id": "FPNMOjCYyCzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boilerplate\n",
        "\n",
        "Just the next few code cells in this section. This is library code, nothing to be changed here."
      ],
      "metadata": {
        "id": "lWTbtaKG9yi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright    2021  Xiaomi Corp.        (authors: Fangjun Kuang)\n",
        "#\n",
        "# See ../../../../LICENSE for clarification regarding multiple authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Implement the LabelSmoothingLoss proposed in the following paper\n",
        "    https://arxiv.org/pdf/1512.00567.pdf\n",
        "    (Rethinking the Inception Architecture for Computer Vision)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        ignore_index: int = -1,\n",
        "        label_smoothing: float = 0.1,\n",
        "        reduction: str = \"sum\",\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          ignore_index:\n",
        "            ignored class id\n",
        "          label_smoothing:\n",
        "            smoothing rate (0.0 means the conventional cross entropy loss)\n",
        "          reduction:\n",
        "            It has the same meaning as the reduction in\n",
        "            `torch.nn.CrossEntropyLoss`. It can be one of the following three\n",
        "            values: (1) \"none\": No reduction will be applied. (2) \"mean\": the\n",
        "            mean of the output is taken. (3) \"sum\": the output will be summed.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert 0.0 <= label_smoothing < 1.0, f\"{label_smoothing}\"\n",
        "        assert reduction in (\"none\", \"sum\", \"mean\"), reduction\n",
        "        self.ignore_index = ignore_index\n",
        "        self.label_smoothing = label_smoothing\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute loss between x and target.\n",
        "\n",
        "        Args:\n",
        "          x:\n",
        "            prediction of dimension\n",
        "            (batch_size, input_length, number_of_classes).\n",
        "          target:\n",
        "            target masked with self.ignore_index of\n",
        "            dimension (batch_size, input_length).\n",
        "\n",
        "        Returns:\n",
        "          A scalar tensor containing the loss without normalization.\n",
        "        \"\"\"\n",
        "        assert x.ndim == 3\n",
        "        assert target.ndim == 2\n",
        "        assert x.shape[:2] == target.shape\n",
        "        num_classes = x.size(-1)\n",
        "        x = x.reshape(-1, num_classes)\n",
        "        # Now x is of shape (N*T, C)\n",
        "\n",
        "        # We don't want to change target in-place below,\n",
        "        # so we make a copy of it here\n",
        "        target = target.clone().reshape(-1)\n",
        "\n",
        "        ignored = target == self.ignore_index\n",
        "\n",
        "        # See https://github.com/k2-fsa/icefall/issues/240\n",
        "        # and https://github.com/k2-fsa/icefall/issues/297\n",
        "        # for why we don't use target[ignored] = 0 here\n",
        "        target = torch.where(ignored, torch.zeros_like(target), target)\n",
        "\n",
        "        true_dist = torch.nn.functional.one_hot(target, num_classes=num_classes).to(x)\n",
        "\n",
        "        true_dist = (\n",
        "            true_dist * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
        "        )\n",
        "\n",
        "        # Set the value of ignored indexes to 0\n",
        "        #\n",
        "        # See https://github.com/k2-fsa/icefall/issues/240\n",
        "        # and https://github.com/k2-fsa/icefall/issues/297\n",
        "        # for why we don't use true_dist[ignored] = 0 here\n",
        "        true_dist = torch.where(\n",
        "            ignored.unsqueeze(1).repeat(1, true_dist.shape[1]),\n",
        "            torch.zeros_like(true_dist),\n",
        "            true_dist,\n",
        "        )\n",
        "\n",
        "        loss = -1 * (torch.log_softmax(x, dim=1) * true_dist)\n",
        "        if self.reduction == \"sum\":\n",
        "            return loss.sum()\n",
        "        elif self.reduction == \"mean\":\n",
        "            return loss.sum() / (~ignored).sum()\n",
        "        else:\n",
        "            return loss.sum(dim=-1)\n"
      ],
      "metadata": {
        "id": "QT5p5wyT9kDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY\n",
        "# Copyright    2023  Xiaomi Corp.        (authors: Xiaoyu Yang)\n",
        "#              2024  Yuekai Zhang\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from typing import List, Any\n",
        "from torch import Tensor\n",
        "from torch.nn.functional import pad as pad_tensor\n",
        "\n",
        "\n",
        "def batch_tensors(tensors: List[Tensor], pad_value: Any) -> Tensor:\n",
        "    padding_size = max(tensor.shape[0] for tensor in tensors)\n",
        "    dims = len(tensors[0].shape)\n",
        "    padded_tensors = []\n",
        "    for tensor in tensors:\n",
        "        padding = [0] * 2 * dims\n",
        "        padding[-1] = padding_size - tensor.shape[0]\n",
        "        padded_tensors.append(pad_tensor(tensor, padding, \"constant\", pad_value))\n",
        "    return torch.stack([tensor for tensor in padded_tensors], dim=0)"
      ],
      "metadata": {
        "id": "3BmUPHKY9qzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaldialign"
      ],
      "metadata": {
        "id": "wzM3FE8F99jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY THIS CODE\n",
        "# Code modified from Icefall: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\n",
        "# Copyright      2021  Xiaomi Corp.        (authors: Fangjun Kuang,\n",
        "#                                                    Mingshuang Luo,\n",
        "#                                                    Zengwei Yao)\n",
        "#\n",
        "# See ../../LICENSE for clarification regarding multiple authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from collections import defaultdict\n",
        "import kaldialign\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "def get_error_stats(\n",
        "    results: List[Tuple[str, str]],\n",
        "    compute_CER: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"Write statistics based on predicted results and reference transcripts.\n",
        "\n",
        "    It will write the following to the given file:\n",
        "\n",
        "        - WER\n",
        "        - number of insertions, deletions, substitutions, corrects and total\n",
        "          reference words. For example::\n",
        "\n",
        "              Errors: 23 insertions, 57 deletions, 212 substitutions, over 2606\n",
        "              reference words (2337 correct)\n",
        "\n",
        "        - The difference between the reference transcript and predicted result.\n",
        "          An instance is given below::\n",
        "\n",
        "            THE ASSOCIATION OF (EDISON->ADDISON) ILLUMINATING COMPANIES\n",
        "\n",
        "          The above example shows that the reference word is `EDISON`,\n",
        "          but it is predicted to `ADDISON` (a substitution error).\n",
        "\n",
        "          Another example is::\n",
        "\n",
        "            FOR THE FIRST DAY (SIR->*) I THINK\n",
        "\n",
        "          The reference word `SIR` is missing in the predicted\n",
        "          results (a deletion error).\n",
        "      results:\n",
        "        An iterable of tuples. The first element is the cut_id, the second is\n",
        "        the reference transcript and the third element is the predicted result.\n",
        "      enable_log:\n",
        "        If True, also print detailed WER to the console.\n",
        "        Otherwise, it is written only to the given file.\n",
        "    Returns:\n",
        "      Return None.\n",
        "    \"\"\"\n",
        "    subs: Dict[Tuple[str, str], int] = defaultdict(int)\n",
        "    ins: Dict[str, int] = defaultdict(int)\n",
        "    dels: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "    # `words` stores counts per word, as follows:\n",
        "    #   corr, ref_sub, hyp_sub, ins, dels\n",
        "    words: Dict[str, List[int]] = defaultdict(lambda: [0, 0, 0, 0, 0])\n",
        "    num_corr = 0\n",
        "    ERR = \"*\"\n",
        "\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR, sclite_mode=False)\n",
        "        for ref_word, hyp_word in ali:\n",
        "            if ref_word == ERR:\n",
        "                ins[hyp_word] += 1\n",
        "                words[hyp_word][3] += 1\n",
        "            elif hyp_word == ERR:\n",
        "                dels[ref_word] += 1\n",
        "                words[ref_word][4] += 1\n",
        "            elif hyp_word != ref_word:\n",
        "                subs[(ref_word, hyp_word)] += 1\n",
        "                words[ref_word][1] += 1\n",
        "                words[hyp_word][2] += 1\n",
        "            else:\n",
        "                words[ref_word][0] += 1\n",
        "                num_corr += 1\n",
        "    ref_len = sum([len(r) for _, r, _ in results])\n",
        "    sub_errs = sum(subs.values())\n",
        "    ins_errs = sum(ins.values())\n",
        "    del_errs = sum(dels.values())\n",
        "    tot_errs = sub_errs + ins_errs + del_errs\n",
        "    tot_err_rate = \"%.2f\" % (100.0 * tot_errs / ref_len)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"%WER {tot_errs / ref_len:.2%} \"\n",
        "        f\"[{tot_errs} / {ref_len}, {ins_errs} ins, \"\n",
        "        f\"{del_errs} del, {sub_errs} sub ]\"\n",
        "    )\n",
        "\n",
        "    print(f\"%WER = {tot_err_rate}\")\n",
        "    print(\n",
        "        f\"Errors: {ins_errs} insertions, {del_errs} deletions, \"\n",
        "        f\"{sub_errs} substitutions, over {ref_len} reference \"\n",
        "        f\"words ({num_corr} correct)\",\n",
        "    )\n",
        "    print(\n",
        "        \"Search below for sections starting with PER-UTT DETAILS:, \"\n",
        "        \"SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\",\n",
        "    )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-UTT DETAILS: corr or (ref->hyp)  \")\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR)\n",
        "        combine_successive_errors = True\n",
        "        if combine_successive_errors:\n",
        "            ali = [[[x], [y]] for x, y in ali]\n",
        "            for i in range(len(ali) - 1):\n",
        "                if ali[i][0] != ali[i][1] and ali[i + 1][0] != ali[i + 1][1]:\n",
        "                    ali[i + 1][0] = ali[i][0] + ali[i + 1][0]\n",
        "                    ali[i + 1][1] = ali[i][1] + ali[i + 1][1]\n",
        "                    ali[i] = [[], []]\n",
        "            ali = [\n",
        "                [\n",
        "                    list(filter(lambda a: a != ERR, x)),\n",
        "                    list(filter(lambda a: a != ERR, y)),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "            ali = list(filter(lambda x: x != [[], []], ali))\n",
        "            ali = [\n",
        "                [\n",
        "                    ERR if x == [] else \" \".join(x),\n",
        "                    ERR if y == [] else \" \".join(y),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "\n",
        "        print(\n",
        "            f\"{cut_id}:\\t\"\n",
        "            + \" \".join(\n",
        "                (\n",
        "                    ref_word if ref_word == hyp_word else f\"({ref_word}->{hyp_word})\"\n",
        "                    for ref_word, hyp_word in ali\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"SUBSTITUTIONS: count ref -> hyp\")\n",
        "\n",
        "    for count, (ref, hyp) in sorted([(v, k) for k, v in subs.items()], reverse=True):\n",
        "        print(f\"{count}   {ref} -> {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"DELETIONS: count ref\")\n",
        "    for count, ref in sorted([(v, k) for k, v in dels.items()], reverse=True):\n",
        "        print(f\"{count}   {ref}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"INSERTIONS: count hyp\")\n",
        "    for count, hyp in sorted([(v, k) for k, v in ins.items()], reverse=True):\n",
        "        print(f\"{count}   {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\")\n",
        "    for _, word, counts in sorted(\n",
        "        [(sum(v[1:]), k, v) for k, v in words.items()], reverse=True\n",
        "    ):\n",
        "        (corr, ref_sub, hyp_sub, ins, dels) = counts\n",
        "        tot_errs = ref_sub + hyp_sub + ins + dels\n",
        "        ref_count = corr + ref_sub + dels\n",
        "        hyp_count = corr + hyp_sub + ins\n",
        "\n",
        "        print(f\"{word}   {corr} {tot_errs} {ref_count} {hyp_count}\")\n",
        "    return float(tot_err_rate)"
      ],
      "metadata": {
        "id": "rCsULWcz9roq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper baseline (0 pt)\n",
        "\n",
        "Write down the WER obtained with the Whisper model in the previous exercise:"
      ],
      "metadata": {
        "id": "XgqQpvil-Gtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mel spectrogram for Whisper (10 pt)\n",
        "\n",
        "The feature extraction for Whisper is done in the code above (see Dataloader section) as follows:\n",
        "\n",
        "```python\n",
        "whisper_audio.log_mel_spectrogram(audio_file, n_mels=..., padding=padding_samples)\n",
        "```\n",
        "\n",
        "where ``audio_file`` is path to an audio file.\n",
        "\n",
        "1. What is the value N_SAMPLES? You may find out by printing it, or referring to the Whisper source code.\n",
        "2. What does it refer to?\n",
        "3. Why is the padding even required?\n",
        "4. What are the advantages and disadvantages of padding especially during model training?\n",
        "5. What do we need to modify in order to avoid padding?"
      ],
      "metadata": {
        "id": "lPNRYgqD-dP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune for a subset of the AMI train set"
      ],
      "metadata": {
        "id": "Y5wD0sv--yV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the small model\n",
        "model = ..."
      ],
      "metadata": {
        "id": "HK5kvNZ--c3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the start of the transcript for training (5 pt)"
      ],
      "metadata": {
        "id": "If-Wj_Nr-8yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the start of sentence token in the vocabulary\n",
        "import whisper\n",
        "tokenizer = whisper.tokenizer.get_tokenizer(\n",
        "        True,\n",
        "        num_languages=model.num_languages,\n",
        "        language=...,\n",
        "        task=...,\n",
        "    )\n",
        "\n",
        "# Find the token ids of the the text \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\"\n",
        "# How?\n",
        "# Method 1: Use dir(tokenizer) to find the member that represents the token ids\n",
        "# Method 2:  use the function mentioned in the previous exercise to find the token ids of each\n",
        "# of the special tokens and put them together in a list to represent them as a sequence\n",
        "# of special tokens.\n",
        "tokens_to_prepend = ...\n",
        "tokens_to_prepend"
      ],
      "metadata": {
        "id": "qLBbZcfK9vkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# justify that tokens_to_prepend is indeed \"<|startoftranscript|><|en|><|transcribe|><|notimestamps|>\""
      ],
      "metadata": {
        "id": "YvdDolxxYBPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Whisper to AMI dataset (15 pt)\n",
        "\n",
        "In this section, we will fine-tune by only using CPUs. We will load run the training with batch size of 1, but will only update the model parameters every 8 batches, making the batch size effectively 8.\n",
        "\n",
        "This is done to avoid running out of memory while training.\n",
        "\n",
        "There are about 1400+ audio files in training. You may want to run the training at least run it on 25% of the files. So, modify the break statement below to stop at an appropriate point such that it doesn't take too long for you to run the training, but run it on at least 25% of the training data."
      ],
      "metadata": {
        "id": "D2XUo3Nt_JDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set seed\n",
        "torch.manual_seed(42)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "model.train()\n",
        "# model = model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "for batch_idx, (feats, texts) in enumerate(train_loader):\n",
        "    # after few batches, stop\n",
        "    ...\n",
        "    # normalize and then convert text to tensor of indices using the model's tokenizer\n",
        "    text_tokens_list = [\n",
        "        tokens_to_prepend\n",
        "        + tokenizer.encode(text.lower())\n",
        "        + [tokenizer.eot]\n",
        "        for text in texts\n",
        "    ]\n",
        "    # convert it to torch tensor\n",
        "    text_tokens_list = [\n",
        "        torch.LongTensor(text_tokens) for text_tokens in text_tokens_list\n",
        "    ]\n",
        "\n",
        "\n",
        "    # 50256 is the index of <pad> for all whisper models\n",
        "    prev_outputs_tokens = batch_tensors(\n",
        "        [tokens[:-1] for tokens in text_tokens_list], pad_value=50256\n",
        "    )\n",
        "    target_tokens = batch_tensors(\n",
        "        [tokens[1:] for tokens in text_tokens_list], pad_value=50256\n",
        "    )\n",
        "    target_lengths = torch.LongTensor(\n",
        "        [tokens.shape[0] - 1 for tokens in text_tokens_list]\n",
        "    )\n",
        "\n",
        "\n",
        "    model.zero_grad()\n",
        "    # ignore the first 3 tokens, which are always <|lang_id|>, <|transcibe|>, <|notimestamps|>\n",
        "    ignore_prefix_size = 3\n",
        "    decoder_criterion = LabelSmoothingLoss(\n",
        "        ignore_index=50256, label_smoothing=0.1, reduction=\"sum\"\n",
        "    )\n",
        "\n",
        "    #feats = feats.cuda()\n",
        "    encoder_out = model.encoder(feats)\n",
        "    #prev_outputs_tokens = prev_outputs_tokens.cuda()\n",
        "    text_logits = model.decoder(prev_outputs_tokens, encoder_out)\n",
        "    text_logits = text_logits[:, ignore_prefix_size:, :]\n",
        "    target_tokens = target_tokens[:, ignore_prefix_size:]\n",
        "    #target_tokens = target_tokens.cuda()\n",
        "    loss = decoder_criterion(text_logits, target_tokens)\n",
        "    loss_value = loss.item()\n",
        "    # normalize loss value by number of sequences in the batch and number of\n",
        "    # frames used as input per batch\n",
        "    loss_value_normalized = ...\n",
        "    print(f\"batch_idx: {batch_idx}, loss: {loss_value_normalized}\")\n",
        "    loss.backward()\n",
        "    if batch_idx % 8 == 0:\n",
        "      optimizer.step()\n",
        "\n",
        "# you may want to add code here if there are some examples\n",
        "# on which loss.backward() was applied, but optimizer.step() was not\n",
        "# run because you exited the loop earlier than expected.\n",
        "..."
      ],
      "metadata": {
        "id": "EAGjacNh_Bwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimate WER with model ( 5pt)\n",
        "\n",
        "Use get_error_stats as done in the previous exercise to display the WER and detailed errors."
      ],
      "metadata": {
        "id": "VG867KCrFYSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "k1VPB_tGFcfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save('./model_fullfinetuning.pt', model.state_dict())"
      ],
      "metadata": {
        "id": "8Bw6_JQiPrJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to load the model again if you need it for later experiments, simply do\n",
        "\n",
        "```python\n",
        "model = load_model(...)  #. we already did this a couple of times earlier\n",
        "model.load_state_dict(torch.load('./model_fullfinetuning.pt'))\n",
        "```"
      ],
      "metadata": {
        "id": "AdNurGkWP1cw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune Whisper on GPUs (10 pt)\n",
        "\n",
        "Run the fine-tuning code above with the statements containing ``.cuda()`` uncommented. Explain the result of the run."
      ],
      "metadata": {
        "id": "_z_mE63iHDp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%% time\n",
        "# add training code here"
      ],
      "metadata": {
        "id": "Pc1ol909HSKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear memory\n",
        "model = None"
      ],
      "metadata": {
        "id": "X2tkAOdJHlPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune with LoRA (20 pt)"
      ],
      "metadata": {
        "id": "H3ToflRoAtEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft"
      ],
      "metadata": {
        "id": "ohsasTNH_Pvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "import peft\n",
        "import whisper\n",
        "model_lora = load_model('small', device='cpu', download_root='./')\n",
        "target_modules = [n for n, m in model_lora.named_modules() if type(m) == whisper.model.Linear]\n",
        "lora_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias=False,\n",
        "    target_modules=target_modules,\n",
        ")\n",
        "model_lora = peft.add_lora(model_lora, lora_config)"
      ],
      "metadata": {
        "id": "KOT3EaZDHdWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the training on model_lora for the same amount of data as done for the full-finetuning. Run the training on GPU, if it doesn't work, you may run it on CPU. Then, evaluate the model on the same test set as before. Also, save the model if it performed better than full-finetuning."
      ],
      "metadata": {
        "id": "ZzqHy4W0Kl0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Number of parameters (10 pt)\n",
        "\n",
        "You may refer to the Whisper paper to answer the following questions. You may also use the ``model`` variable to answer the questions.\n",
        "\n",
        "1. How many parameters does Whisper small have?\n",
        "2. How many layers of self-attention based transformer module does the Whisper encoder have for the 'small' model?\n",
        "3. How many layers does the Whisper decoder have for the 'small' model?\n",
        "4. How many linear layers (i.e. nn.Linear) does the Whisper small model have in total?\n",
        "5. Compute the total number of new parameters that we added with LoRA by peft.add_lora() code earlier given the answer for for 4 and given that LoRA rank is 64."
      ],
      "metadata": {
        "id": "yEfROC0kK-o3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transducer model with Whisper encoder (5 pt)"
      ],
      "metadata": {
        "id": "XeM9qE55Mcr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Joiner(nn.Module):\n",
        "    def __init__(self):\n",
        "      ...\n",
        "    def forward(self):\n",
        "      ...\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self):\n",
        "      ...\n",
        "    def forward(self):\n",
        "      ...\n",
        "\n",
        "class WhisperTransducer(nn.Module):\n",
        "    def __init__(self, encoder, tokenizer):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.joiner = Joiner()  # assume Joiner has been already defined as in https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless7/joiner.py\n",
        "        self.predictor = Predictor()  # assume Predictor has been already defined as in https://github.com/k2-fsa/icefall/blob/master/egs/librispeech/ASR/pruned_transducer_stateless7/decoder.py\n",
        "\n",
        "    def forward(self, audio_file):\n",
        "        x = self.encoder(x)\n",
        "        # rest of transducer logic (not required to fill)\n",
        "        ...\n",
        "        return x"
      ],
      "metadata": {
        "id": "oZQyS079KsGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: extract the encoder only from whisper model and create\n",
        "# an instance of WhisperTransducer by passing the encoder"
      ],
      "metadata": {
        "id": "62kcg4o8OJ9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best performance on the test data (5 pt)\n",
        "\n",
        "How can one achieve a lower bound of the WER on a given data set? This is one way to know how far we can keep improving a given model's performance on that dataset. The 'model' here is not restricted to Whisper. It could by any type of ASR system."
      ],
      "metadata": {
        "id": "K5_jlpjjOwOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word boosting for Whisper (15 pt)\n",
        "\n",
        "Suppose we want to avoid some common mistakes, e.g. we know that the speech has 'wanna' but the ASR outputs 'want to'. Identify one such simple mistake in the result of your best model. Identify all the files (you may do it manually since there are only 150 test files), and add the correct word/phrase to be used to the prompt when decoding the audio where you want to avoid such confusion.\n",
        "\n",
        "When decoding the prompt can be passed with the ``initial_prompt`` argument.\n",
        "\n",
        "Show the output before and after the word boosting. Explain clearly (in technical terms) what makes the boosting work/not work in your case."
      ],
      "metadata": {
        "id": "wW5VSSLwT0F7"
      }
    }
  ]
}