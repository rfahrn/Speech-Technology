{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fricRbYq0Iqi"
      },
      "source": [
        "# Exercise 5\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Make sure you have uploaded the audio files to Google Drive.\n",
        "- Please read the markdown sections, and code comments carefully before answering.\n",
        "- You are required to treat ``...`` as incomplete code, which you are required to complete.\n",
        "- Each incomplete region marked by ``...`` can be completed with a maximum of 2 statements (2 lines of code in Python).\n",
        "- You may refer to the slides and reference material, but may not use AI code completion.\n",
        "- Run all code cells in the notebook even if it does not require any answer from your part.\n",
        "- The point for each section or sub-section is given in square brackets. E.g [15 pt] means 15 points.\n",
        "- Pay attention to Q. & A. questions. The markdown-python cell separation is not always obvious.\n",
        "- **ATTENTION**: There are many places where the path of the audio file needs to be fixed by you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlBEs5gZz25Y"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBqsRBFz26SZ"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSE4jc_C3DFb"
      },
      "outputs": [],
      "source": [
        "#!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1bam2hZTOpPf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# jiwer example usage\n",
        "import jiwer\n",
        "reference = \"hello world\"\n",
        "hypothesis = \"hello duck\"\n",
        "\n",
        "error = jiwer.wer(reference, hypothesis)\n",
        "error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bocPy_DMdzeO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.45454545454545453"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "character_error = jiwer.cer(reference, hypothesis)\n",
        "character_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zpeHbC6WSaK1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg3DNBza0m-Z"
      },
      "source": [
        "## 1. Word Error Rate (20pt)\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. Use jiwer to get WER and CER between the following hypothesis and reference (5)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8q_U8hrRbgeR"
      },
      "outputs": [],
      "source": [
        "ref = 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE'\n",
        "hyp = 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FATTEN SAUCE'\n",
        "wer = jiwer.wer(ref, hyp)\n",
        "cer = jiwer.cer(ref, hyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS292rUbsQe"
      },
      "source": [
        "2. Give example of an ASR output with WER 10% with only one of the following errors: insertions, deletions, or substitutions w.r.t the following reference? (5)\n",
        "\n",
        "Reference is:\n",
        "\n",
        "THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYPvGxoMcEak"
      },
      "source": [
        "3.  Give example of an ASR output with WER 150% w.r.t the following reference? (5)\n",
        "\n",
        "Reference is:\n",
        "\n",
        "THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npXGovzKdW3c"
      },
      "source": [
        "4. A dataset has 10 speech utterances. We are also given reference text for each of those 10 utterances. Given only WER of an ASR system for each of the 10 utterances (i.e. we do not have access to the hypothesis of the ASR system), how can we calculate the WER on the entire dataset ? (5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FbcetEP0o6q"
      },
      "source": [
        "## 2. Decoding with Whisper\n",
        "\n",
        "In this section we will decode with the [AMI dataset](https://groups.inf.ed.ac.uk/ami/corpus/). We will use a subset of the test split of the dataset. We will use the term \"AMI test\" to refer to this set.\n",
        "\n",
        "There are three tasks in this section:\n",
        "\n",
        "1. Decode on AMI test with default parameters and evaluate the model's performance.\n",
        "2. Decode on AMI test with beam search with beam size 4 and evaluate the model's performance.\n",
        "3. Evaluate WER before and after text normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rmYKquRtNs"
      },
      "source": [
        "## 2.1 Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zzsDz8JKPgbx"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 2] Das System kann die angegebene Datei nicht finden",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, download_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Whisper uses a greedy decoder when no option is specified.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#result_greedy = model.transcribe(\"/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/audio_files_ex5/ami-en2002b/EN2002b-3-379-ihm.wav\", language=\"en\", temperature=0)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m result_greedy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ami-en2002b/EN2002b-3-379-ihm.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m result_greedy[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\site-packages\\whisper\\transcribe.py:139\u001b[0m, in \u001b[0;36mtranscribe\u001b[1;34m(model, audio, verbose, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, condition_on_previous_text, initial_prompt, carry_initial_prompt, word_timestamps, prepend_punctuations, append_punctuations, clip_timestamps, hallucination_silence_threshold, **decode_options)\u001b[0m\n\u001b[0;32m    136\u001b[0m     decode_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Pad 30-seconds of silence to the input audio, for slicing\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m mel \u001b[38;5;241m=\u001b[39m \u001b[43mlog_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m content_frames \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m N_FRAMES\n\u001b[0;32m    141\u001b[0m content_duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(content_frames \u001b[38;5;241m*\u001b[39m HOP_LENGTH \u001b[38;5;241m/\u001b[39m SAMPLE_RATE)\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\site-packages\\whisper\\audio.py:140\u001b[0m, in \u001b[0;36mlog_mel_spectrogram\u001b[1;34m(audio, n_mels, padding, device)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(audio):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 140\u001b[0m         audio \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     audio \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(audio)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\site-packages\\whisper\\audio.py:58\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file, sr)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstdout\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load audio: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mdecode()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    500\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m    501\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    505\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    968\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    969\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
            "File \u001b[1;32mc:\\Users\\rebec\\miniconda3\\envs\\GPU-env\\lib\\subprocess.py:1456\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1454\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1456\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1457\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1458\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1470\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1472\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1473\u001b[0m                          errread, errwrite)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Das System kann die angegebene Datei nicht finden"
          ]
        }
      ],
      "source": [
        "from whisper import load_model\n",
        "model = load_model('small', device='cpu', download_root='./')\n",
        "# Whisper uses a greedy decoder when no option is specified.\n",
        "#result_greedy = model.transcribe(\"/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/audio_files_ex5/ami-en2002b/EN2002b-3-379-ihm.wav\", language=\"en\", temperature=0)\n",
        "result_greedy = model.transcribe(\"/ami-en2002b/EN2002b-3-379-ihm.wav\", language=\"en\", temperature=0)\n",
        "result_greedy['text']  # way to access hypothesis after decoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09q2TK2cUYcI"
      },
      "source": [
        "## 2.2 Whisper decoding (5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYqoLwe8dJcV"
      },
      "outputs": [],
      "source": [
        "utt2ref = {}\n",
        "# TODO: fix path\n",
        "with open('/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/audio_files_ex5/ami-en2002b/text_random150') as ipf:\n",
        "  for ln in ipf:\n",
        "    utt, *text = ln.strip().split()\n",
        "    utt2ref[utt] = \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "90xFeD4ScAiU"
      },
      "outputs": [],
      "source": [
        "utt2hyp = {}\n",
        "# for each utterance in utt2ref, decode and store the result in utt2hyp[utt]\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYPjvdnpdadV"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for utt in utt2ref:\n",
        "  if utt not in utt2hyp:\n",
        "    print(f\"ERROR: Missing hypothesis for utt {utt}\")\n",
        "    break\n",
        "  results.append((utt, utt2ref[utt].split(), utt2hyp[utt].split()))\n",
        "results[0] # example showing what results contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aOwCn9YedG2"
      },
      "outputs": [],
      "source": [
        "!pip install kaldialign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWttJA0UdjOb"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CODE\n",
        "# Code modified from Icefall: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\n",
        "# Copyright      2021  Xiaomi Corp.        (authors: Fangjun Kuang,\n",
        "#                                                    Mingshuang Luo,\n",
        "#                                                    Zengwei Yao)\n",
        "#\n",
        "# See ../../LICENSE for clarification regarding multiple authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from collections import defaultdict\n",
        "import kaldialign\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "def get_error_stats(\n",
        "    results: List[Tuple[str, str]],\n",
        "    compute_CER: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"Write statistics based on predicted results and reference transcripts.\n",
        "\n",
        "    It will write the following to the given file:\n",
        "\n",
        "        - WER\n",
        "        - number of insertions, deletions, substitutions, corrects and total\n",
        "          reference words. For example::\n",
        "\n",
        "              Errors: 23 insertions, 57 deletions, 212 substitutions, over 2606\n",
        "              reference words (2337 correct)\n",
        "\n",
        "        - The difference between the reference transcript and predicted result.\n",
        "          An instance is given below::\n",
        "\n",
        "            THE ASSOCIATION OF (EDISON->ADDISON) ILLUMINATING COMPANIES\n",
        "\n",
        "          The above example shows that the reference word is `EDISON`,\n",
        "          but it is predicted to `ADDISON` (a substitution error).\n",
        "\n",
        "          Another example is::\n",
        "\n",
        "            FOR THE FIRST DAY (SIR->*) I THINK\n",
        "\n",
        "          The reference word `SIR` is missing in the predicted\n",
        "          results (a deletion error).\n",
        "      results:\n",
        "        An iterable of tuples. The first element is the cut_id, the second is\n",
        "        the reference transcript and the third element is the predicted result.\n",
        "      enable_log:\n",
        "        If True, also print detailed WER to the console.\n",
        "        Otherwise, it is written only to the given file.\n",
        "    Returns:\n",
        "      Return None.\n",
        "    \"\"\"\n",
        "    subs: Dict[Tuple[str, str], int] = defaultdict(int)\n",
        "    ins: Dict[str, int] = defaultdict(int)\n",
        "    dels: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "    # `words` stores counts per word, as follows:\n",
        "    #   corr, ref_sub, hyp_sub, ins, dels\n",
        "    words: Dict[str, List[int]] = defaultdict(lambda: [0, 0, 0, 0, 0])\n",
        "    num_corr = 0\n",
        "    ERR = \"*\"\n",
        "\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR, sclite_mode=False)\n",
        "        for ref_word, hyp_word in ali:\n",
        "            if ref_word == ERR:\n",
        "                ins[hyp_word] += 1\n",
        "                words[hyp_word][3] += 1\n",
        "            elif hyp_word == ERR:\n",
        "                dels[ref_word] += 1\n",
        "                words[ref_word][4] += 1\n",
        "            elif hyp_word != ref_word:\n",
        "                subs[(ref_word, hyp_word)] += 1\n",
        "                words[ref_word][1] += 1\n",
        "                words[hyp_word][2] += 1\n",
        "            else:\n",
        "                words[ref_word][0] += 1\n",
        "                num_corr += 1\n",
        "    ref_len = sum([len(r) for _, r, _ in results])\n",
        "    sub_errs = sum(subs.values())\n",
        "    ins_errs = sum(ins.values())\n",
        "    del_errs = sum(dels.values())\n",
        "    tot_errs = sub_errs + ins_errs + del_errs\n",
        "    tot_err_rate = \"%.2f\" % (100.0 * tot_errs / ref_len)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"%WER {tot_errs / ref_len:.2%} \"\n",
        "        f\"[{tot_errs} / {ref_len}, {ins_errs} ins, \"\n",
        "        f\"{del_errs} del, {sub_errs} sub ]\"\n",
        "    )\n",
        "\n",
        "    print(f\"%WER = {tot_err_rate}\")\n",
        "    print(\n",
        "        f\"Errors: {ins_errs} insertions, {del_errs} deletions, \"\n",
        "        f\"{sub_errs} substitutions, over {ref_len} reference \"\n",
        "        f\"words ({num_corr} correct)\",\n",
        "    )\n",
        "    print(\n",
        "        \"Search below for sections starting with PER-UTT DETAILS:, \"\n",
        "        \"SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\",\n",
        "    )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-UTT DETAILS: corr or (ref->hyp)  \")\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR)\n",
        "        combine_successive_errors = True\n",
        "        if combine_successive_errors:\n",
        "            ali = [[[x], [y]] for x, y in ali]\n",
        "            for i in range(len(ali) - 1):\n",
        "                if ali[i][0] != ali[i][1] and ali[i + 1][0] != ali[i + 1][1]:\n",
        "                    ali[i + 1][0] = ali[i][0] + ali[i + 1][0]\n",
        "                    ali[i + 1][1] = ali[i][1] + ali[i + 1][1]\n",
        "                    ali[i] = [[], []]\n",
        "            ali = [\n",
        "                [\n",
        "                    list(filter(lambda a: a != ERR, x)),\n",
        "                    list(filter(lambda a: a != ERR, y)),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "            ali = list(filter(lambda x: x != [[], []], ali))\n",
        "            ali = [\n",
        "                [\n",
        "                    ERR if x == [] else \" \".join(x),\n",
        "                    ERR if y == [] else \" \".join(y),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "\n",
        "        print(\n",
        "            f\"{cut_id}:\\t\"\n",
        "            + \" \".join(\n",
        "                (\n",
        "                    ref_word if ref_word == hyp_word else f\"({ref_word}->{hyp_word})\"\n",
        "                    for ref_word, hyp_word in ali\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"SUBSTITUTIONS: count ref -> hyp\")\n",
        "\n",
        "    for count, (ref, hyp) in sorted([(v, k) for k, v in subs.items()], reverse=True):\n",
        "        print(f\"{count}   {ref} -> {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"DELETIONS: count ref\")\n",
        "    for count, ref in sorted([(v, k) for k, v in dels.items()], reverse=True):\n",
        "        print(f\"{count}   {ref}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"INSERTIONS: count hyp\")\n",
        "    for count, hyp in sorted([(v, k) for k, v in ins.items()], reverse=True):\n",
        "        print(f\"{count}   {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\")\n",
        "    for _, word, counts in sorted(\n",
        "        [(sum(v[1:]), k, v) for k, v in words.items()], reverse=True\n",
        "    ):\n",
        "        (corr, ref_sub, hyp_sub, ins, dels) = counts\n",
        "        tot_errs = ref_sub + hyp_sub + ins + dels\n",
        "        ref_count = corr + ref_sub + dels\n",
        "        hyp_count = corr + hyp_sub + ins\n",
        "\n",
        "        print(f\"{word}   {corr} {tot_errs} {ref_count} {hyp_count}\")\n",
        "    return float(tot_err_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWlAHt34ktNF"
      },
      "source": [
        "Run the above code. Check the most common errors (insertion, substituion, deletion) and propose some methods to fix the issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0EirLAkZS2VW"
      },
      "outputs": [],
      "source": [
        "get_error_stats(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxSZskIk0RM"
      },
      "source": [
        "### Text normalization (20 pt)\n",
        "\n",
        "Whisper outputs human readable text by default. Notice that the this is not the case for reference.\n",
        "\n",
        "Normalize the reference and/or hypothesis text output in a way that both of them match. Then compute the WER again with ``get_error_stats()``.\n",
        "\n",
        "You may Google for any Python-relevant help for string processing. If you use an external reference, please add the reference in the comment.\n",
        "\n",
        "You may reuse code from previous sections to compute WER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4rTYKdhkzxC"
      },
      "outputs": [],
      "source": [
        "# Enter your code here. You can use multiple cells if necessary.\n",
        "# Code \"may\" be longer than 3 statements in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5ijJw1yVkIJ"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "...\n",
        "\n",
        "results[0] # example showing what results contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DUH6cCaEVwtj"
      },
      "outputs": [],
      "source": [
        "get_error_stats(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLhaHy7GqOpM"
      },
      "source": [
        "## 2.5 Error analysis (5 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToSQOh2dXmkL"
      },
      "source": [
        "Pick two specific types of errors and discuss how we can improve the ASR for such errors. Note that we are not referring to INSERTION, DELETION and SUBSTITUTION errors, but something more specific. For instance, you could mention a particular error like 'TWO -> TO' and propose a remedy. Choose the ASR output with the best WER so far for your analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHmJ23-tkkUJ"
      },
      "source": [
        "### 2.6 Plot WER vs utterance duration (10pt)\n",
        "\n",
        "Create a plot to check if the duration of the utterance is related to the WER. Conclude upon visual observation if there exists a correlation between the two quantities. Use the system with the best WER so far. Justify your response."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzA7rKZi4wr4"
      },
      "source": [
        "## 3. Whisper tokenizer (15pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSY5OEZeLmC4"
      },
      "source": [
        "In the next code cell, the tokenizer used by multilingual models is initialized in the variable ``tokenizer``. Without changing the ``tokenizer`` variable, do the following:\n",
        "\n",
        "1. Print the token index of beginning of sentence and end of sentence special tokens\n",
        "2. Based on [this dictionary](https://github.com/openai/whisper/blob/90db0de1896c23cbfaf0c58bc2d30665f709f170/whisper/tokenizer.py#L10), print the the token index of two languages: French and German. An example is already given below for English.\n",
        "3. Given that the supported languages are in ``whisper.tokenizer.LANGUAGES``, find out only using python code if (a) your native tongue is supported (b) the language Kurmanji is supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiq1IRCR3V4p"
      },
      "outputs": [],
      "source": [
        "from whisper import load_model\n",
        "\n",
        "model = load_model('base', device='cpu', download_root='./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T56NbrBr4wXS"
      },
      "outputs": [],
      "source": [
        "import whisper.tokenizer as whisper_tokenizer\n",
        "\n",
        "tokenizer = whisper_tokenizer.get_tokenizer(\n",
        "    True,  # assume a multilingual model\n",
        "    num_languages=model.num_languages,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG_66a2Q30WH"
      },
      "outputs": [],
      "source": [
        "## 1. Print the token index of bos, eos (2.5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XtqMe7Do37qf"
      },
      "outputs": [],
      "source": [
        "# This is an example to get the language token for English.\n",
        "tokenizer.special_tokens.get('<|en|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbTGW92Z5gsL"
      },
      "outputs": [],
      "source": [
        "## 2. print the the token index of French and German (2.5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4k5jTHa5rQc"
      },
      "outputs": [],
      "source": [
        "## 3. Find out only using python code if (a) your native tongue is supported (b) the language Kurmanji is supported. (10 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su9OeUZ0VzpK"
      },
      "source": [
        "## 1D convolution vs TDNN (25pt)\n",
        "\n",
        "As seen in class, Whisper uses two convolutional layers before the Transformer layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUehV40XV7HN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "n_mels = 80\n",
        "n_state = 768\n",
        "conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3)\n",
        "conv2 = nn.Conv1d(n_state, n_state, kernel_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7RCH-i4eg9d"
      },
      "outputs": [],
      "source": [
        "# 1. Create an appropriate input for the above convolutional layer, and apply it\n",
        "# to the first convolutional layer. The input is a 3-dimensional tensor.\n",
        "# The first dimenion is a number of sequences, we assume there is only one sequence.\n",
        "# The second dimenion is the number of filterbank energies per frame.\n",
        "# The third dimension is the maximum audio length in number of frames.\n",
        "\n",
        "x = torch.randn(1, n_mels, 3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEt-O6dfhH_0"
      },
      "outputs": [],
      "source": [
        "# 2. Get the output after passing it through the first 2 convolution layers.\n",
        "\n",
        "# forward pass\n",
        "x = F.gelu(conv1(x))\n",
        "x = F.gelu(conv2(x))\n",
        "\n",
        "# debug statement only, not part of forward pass\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkfovmrhiBPf"
      },
      "outputs": [],
      "source": [
        "# 3. Adjust the hyperparameters of the two Conv1d layers above to get an output\n",
        "# of sequence length 1000. Please re-write all code in this cell here. Do not\n",
        "# modify the code above.\n",
        "\n",
        "# conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, bias=False, padding=1)\n",
        "# conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, padding=1, stride=3)\n",
        "...\n",
        "\n",
        "x = torch.randn(1, n_mels, 3000)\n",
        "x = F.gelu(conv1(x))\n",
        "x = F.gelu(conv2(x))\n",
        "x.shape\n",
        "# assert x.shape == torch.Size([1, 768, 150])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQD-eQygpuME"
      },
      "source": [
        "4. Would your approach to getting the desired sequence length at the output of the two convolutional layers change if we did not apply the non-linearity in the forward pass?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKzZmNCqjdNe"
      },
      "source": [
        "5. Use the Conv1D class instead of the TDNN class in Exercise 4 to create one of the layers of the X-vector network. The details about which layer is given in the code below.\n",
        "\n",
        "NOTE: Only re-write it. No need to test it (unless you prefer to demonstrate that they are equal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fw7QlHcj0Oe"
      },
      "outputs": [],
      "source": [
        "# The class is commented. It is only to be used as a reference\n",
        "# class TDNN(nn.Module):\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         feat_dim,\n",
        "#         output_dim,\n",
        "#         context_len=1,\n",
        "#     ):\n",
        "#         super(TDNN, self).__init__()\n",
        "#         self.linear = nn.Linear(feat_dim*context_len, output_dim)\n",
        "#         self.context_len = torch.tensor(context_len, requires_grad=False)\n",
        "\n",
        "\n",
        "#     def forward(self, input):\n",
        "#         mb, T, D = input.shape\n",
        "#         padded_input = input.reshape(mb, -1).unfold(1, D*self.context_len, D).contiguous()\n",
        "#         x = self.linear(padded_input)\n",
        "#         return x\n",
        "\n",
        "# Rewrite the line below with Conv1D\n",
        "# tdnn1 = TDNN(feat_dim=40, output_dim=128, context_len=3)\n",
        "\n",
        "tdnn1 = nn.Conv1D(...)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "GPU-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
