{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Author: \n",
        "\n",
        "Rebecka Fahrni (18-735-522)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fricRbYq0Iqi"
      },
      "source": [
        "# Exercise 5\n",
        "\n",
        "## Instructions\n",
        "\n",
        "- Make sure you have uploaded the audio files to Google Drive.\n",
        "- Please read the markdown sections, and code comments carefully before answering.\n",
        "- You are required to treat ``...`` as incomplete code, which you are required to complete.\n",
        "- Each incomplete region marked by ``...`` can be completed with a maximum of 2 statements (2 lines of code in Python).\n",
        "- You may refer to the slides and reference material, but may not use AI code completion.\n",
        "- Run all code cells in the notebook even if it does not require any answer from your part.\n",
        "- The point for each section or sub-section is given in square brackets. E.g [15 pt] means 15 points.\n",
        "- Pay attention to Q. & A. questions. The markdown-python cell separation is not always obvious.\n",
        "- **ATTENTION**: There are many places where the path of the audio file needs to be fixed by you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "HlBEs5gZz25Y"
      },
      "outputs": [],
      "source": [
        "# mount google drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "pBqsRBFz26SZ"
      },
      "outputs": [],
      "source": [
        "#!pip install git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "KSE4jc_C3DFb"
      },
      "outputs": [],
      "source": [
        "#!pip install jiwer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "1bam2hZTOpPf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# jiwer example usage\n",
        "import jiwer\n",
        "reference = \"hello world\"\n",
        "hypothesis = \"hello duck\"\n",
        "\n",
        "error = jiwer.wer(reference, hypothesis)\n",
        "error\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bocPy_DMdzeO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.45454545454545453"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "character_error = jiwer.cer(reference, hypothesis)\n",
        "character_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "zpeHbC6WSaK1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg3DNBza0m-Z"
      },
      "source": [
        "## 1. Word Error Rate (20pt)\n",
        "\n",
        "Questions:\n",
        "\n",
        "1. Use jiwer to get WER and CER between the following hypothesis and reference (5)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "8q_U8hrRbgeR"
      },
      "outputs": [],
      "source": [
        "ref = 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOUR FATTENED SAUCE'\n",
        "hyp = 'HE HOPED THERE WOULD BE STEW FOR DINNER TURNIPS AND CARROTS AND BRUISED POTATOES AND FAT MUTTON PIECES TO BE LADLED OUT IN THICK PEPPERED FLOWER FATTEN SAUCE'\n",
        "wer = jiwer.wer(ref, hyp)\n",
        "cer = jiwer.cer(ref, hyp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZS292rUbsQe"
      },
      "source": [
        "2. Give example of an ASR output with WER 10% with only one of the following errors: insertions, deletions, or substitutions w.r.t the following reference? (5)\n",
        "\n",
        "Reference is:\n",
        "\n",
        "THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR\n",
        "\n",
        "\n",
        "----\n",
        "Answer: so the total number of words in the reference is 10. So, 10% of 10 is 1 word error. So, the hypothesis should have 9 words correct and 1 word error. The error can be either insertion, deletion, or substitution.\n",
        "For example, the hypothesis can be: \n",
        "\n",
        "- for an inserion error, THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR QUICKLY (added one word), \n",
        "\n",
        "- for a deletion error: THE DARK SUIT WAS IN GREASY WASH WATER ALL (deleted one word), \n",
        "\n",
        "- for a substitution error: THE DARK SUIT WAS IN GREASY WASH WATER ALL MONTH (substituted one word/ year by month).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYPvGxoMcEak"
      },
      "source": [
        "3.  Give example of an ASR output with WER 150% w.r.t the following reference? (5)\n",
        "\n",
        "Reference is:\n",
        "\n",
        "THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR\n",
        "\n",
        "---\n",
        "Answer: so the total number of words in the reference is 10. So, 150% of 10 is 15 word errors.\n",
        "So, in the case of an all insertion error, the hypothesis length is calculated as 10+15=25 words. The hypothesis can be:\n",
        "\n",
        "THE DARK SUIT WAS IN GREASY WASH WATER ALL YEAR SUDDENLY QUICKLY RANDOMLY APPEARED AGAIN\n",
        "\n",
        "in the case of combination of deletion, substitution, and insertion errors, the hypothesis can have also a hypothesis lenght of 8. Where the errors can be (6 substitutions, 4 deletions, 5 insertions):\n",
        "then the hypothesis can be:\n",
        "DARK SUIT WAS IN GREASY WASH WATER ALL YEAR SUDDENLY QUICKLY RANDOMLY APPEARED AGAIN (and be still have a WER of 150%)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npXGovzKdW3c"
      },
      "source": [
        "4. A dataset has 10 speech utterances. We are also given reference text for each of those 10 utterances. Given only WER of an ASR system for each of the 10 utterances (i.e. we do not have access to the hypothesis of the ASR system), how can we calculate the WER on the entire dataset ? (5)\n",
        "\n",
        "---\n",
        "Answer:  overall WER for the dataset is computed as total number of edit errors across all utterances divided by the total number of words in references for all utterances. Since we are given only the WER for each utterance - use formula:\n",
        "$$\n",
        "\\text{WER}_{\\text{dataset}} = \\frac{\\sum_{i=1}^{10} (\\text{WER}_i \\times R_i)}{\\sum_{i=1}^{10} R_i}\n",
        "$$\n",
        "with:\n",
        "- $( R_i)$ is the total number of words in the reference for utterance $( i)$.  \n",
        "- $( \\text{WER}_i)$ is the WER for utterance $( i)$.  \n",
        "- $( \\text{WER}_i \\times R_i)$ gives the total edit errors for utterance $( i )$.\n",
        "\n",
        "so the steps are as follows:\n",
        "1. for each utterance $( i)$, multiply $( \\text{WER}_i)$ (expressed as a fraction) by $( R_i)$ to calculate total edit errors.  \n",
        "2. sum total edit errors for all $( 10 )$ utterances.  \n",
        "3. sum  total number of words in all references $(( \\sum_{i=1}^{10} R_i ))$.  \n",
        "4. Divide the total edit errors by the total reference words to get the overall WER.\n",
        "\n",
        "for example:\n",
        "- $( R_1 = 10, WER_1 = 0.2 )$ (20%)  \n",
        "- $( R_2 = 15, WER_2 = 0.1 )$ (10%)  \n",
        "- $( \\ldots )$  \n",
        "- $( R_{10} = 8, WER_{10} = 0.3 )$ (30%)  \n",
        "then the total edit errors across all utterances is:\n",
        "$$\n",
        "\\text{Edit Errors}_\\text{total} = (0.2 \\times 10) + (0.1 \\times 15) + \\ldots + (0.3 \\times 8)\n",
        "$$\n",
        "\n",
        "and the total reference words:\n",
        "$$\n",
        "\\text{Words}_\\text{total} = R_1 + R_2 + \\ldots + R_{10}\n",
        "$$\n",
        "\n",
        "so dataset WER:\n",
        "$$\n",
        "\\text{WER}_{\\text{dataset}} = \\frac{\\text{Edit Errors}_\\text{total}}{\\text{Words}_\\text{total}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FbcetEP0o6q"
      },
      "source": [
        "## 2. Decoding with Whisper\n",
        "\n",
        "In this section we will decode with the [AMI dataset](https://groups.inf.ed.ac.uk/ami/corpus/). We will use a subset of the test split of the dataset. We will use the term \"AMI test\" to refer to this set.\n",
        "\n",
        "There are three tasks in this section:\n",
        "\n",
        "1. Decode on AMI test with default parameters and evaluate the model's performance.\n",
        "2. Decode on AMI test with beam search with beam size 4 and evaluate the model's performance.\n",
        "3. Evaluate WER before and after text normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rmYKquRtNs"
      },
      "source": [
        "## 2.1 Example usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "path = os.getcwd()\n",
        "audio_dir = path +'/ami-en2002b/ami-en2002b'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "zzsDz8JKPgbx"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "' Well, I thought you had tomorrow.'"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from whisper import load_model\n",
        "model = load_model('small', device='cpu', download_root='./')\n",
        "# Whisper uses a greedy decoder when no option is specified.\n",
        "#result_greedy = model.transcribe(\"/content/drive/MyDrive/work/uzh/teaching/2024-speech-technology/audio_files_ex5/ami-en2002b/EN2002b-3-379-ihm.wav\", language=\"en\", temperature=0)\n",
        "test_audio = audio_dir+ '/EN2002b-3-379-ihm.wav'\n",
        "result_greedy = model.transcribe(test_audio, language=\"en\", temperature=0)\n",
        "result_greedy['text']  # way to access hypothesis after decoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09q2TK2cUYcI"
      },
      "source": [
        "## 2.2 Whisper decoding (5 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "LYqoLwe8dJcV"
      },
      "outputs": [],
      "source": [
        "utt2ref = {}\n",
        "# TODO: fix path\n",
        "with open(audio_dir+ '/text_random150') as ipf:\n",
        "  for ln in ipf:\n",
        "    utt, *text = ln.strip().split()\n",
        "    utt2ref[utt] = \" \".join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "collapsed": true,
        "id": "90xFeD4ScAiU"
      },
      "outputs": [],
      "source": [
        "# Decode with default settings (greedy decoding)\n",
        "utt2hyp = {}\n",
        "# for each utterance in utt2ref, decode and store the result in utt2hyp[utt]\n",
        "for utt in utt2ref:\n",
        "    audio_path = os.path.join(audio_dir, f\"{utt}.wav\")\n",
        "    if os.path.exists(audio_path):  \n",
        "        result = model.transcribe(audio_path, language=\"en\", temperature=0)\n",
        "        utt2hyp[utt] = result['text']\n",
        "    else:\n",
        "        print(f\"WARNING: Audio file not found for utterance {utt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "QYPjvdnpdadV"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('EN2002b-1-170-ihm',\n",
              " ['GOALS', 'OR', 'SOMETHING'],\n",
              " ['goals', 'or', 'something.'])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = []\n",
        "for utt in utt2ref:\n",
        "  if utt not in utt2hyp:\n",
        "    print(f\"ERROR: Missing hypothesis for utt {utt}\")\n",
        "    break\n",
        "  results.append((utt, utt2ref[utt].split(), utt2hyp[utt].split()))\n",
        "results[0] # example showing what results contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('EN2002b-1-170-ihm',\n",
              " ['GOALS', 'OR', 'SOMETHING'],\n",
              " ['goals', 'or', 'something.'])"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# now using Beam Search: \n",
        "# Decode with beam search decoding (beam size 4)\n",
        "utt2hyp_beam = {}\n",
        "for utt in utt2ref:\n",
        "    audio_path = os.path.join(audio_dir, f\"{utt}.wav\")\n",
        "    if os.path.exists(audio_path): \n",
        "        result_beam = model.transcribe(audio_path, language=\"en\", beam_size=4)\n",
        "        utt2hyp_beam[utt] = result_beam['text']\n",
        "        \n",
        "results_beam = []\n",
        "for utt in utt2ref:\n",
        "  if utt not in utt2hyp_beam:\n",
        "    break\n",
        "  results_beam.append((utt, utt2ref[utt].split(), utt2hyp_beam[utt].split()))\n",
        "results_beam[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "8aOwCn9YedG2"
      },
      "outputs": [],
      "source": [
        "#!pip install kaldialign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "CWttJA0UdjOb"
      },
      "outputs": [],
      "source": [
        "# DO NOT MODIFY THIS CODE\n",
        "# Code modified from Icefall: https://github.com/k2-fsa/icefall/blob/master/icefall/utils.py\n",
        "# Copyright      2021  Xiaomi Corp.        (authors: Fangjun Kuang,\n",
        "#                                                    Mingshuang Luo,\n",
        "#                                                    Zengwei Yao)\n",
        "#\n",
        "# See ../../LICENSE for clarification regarding multiple authors\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "from collections import defaultdict\n",
        "import kaldialign\n",
        "from typing import Dict, Tuple, List\n",
        "\n",
        "def get_error_stats(\n",
        "    results: List[Tuple[str, str]],\n",
        "    compute_CER: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"Write statistics based on predicted results and reference transcripts.\n",
        "\n",
        "    It will write the following to the given file:\n",
        "\n",
        "        - WER\n",
        "        - number of insertions, deletions, substitutions, corrects and total\n",
        "          reference words. For example::\n",
        "\n",
        "              Errors: 23 insertions, 57 deletions, 212 substitutions, over 2606\n",
        "              reference words (2337 correct)\n",
        "\n",
        "        - The difference between the reference transcript and predicted result.\n",
        "          An instance is given below::\n",
        "\n",
        "            THE ASSOCIATION OF (EDISON->ADDISON) ILLUMINATING COMPANIES\n",
        "\n",
        "          The above example shows that the reference word is `EDISON`,\n",
        "          but it is predicted to `ADDISON` (a substitution error).\n",
        "\n",
        "          Another example is::\n",
        "\n",
        "            FOR THE FIRST DAY (SIR->*) I THINK\n",
        "\n",
        "          The reference word `SIR` is missing in the predicted\n",
        "          results (a deletion error).\n",
        "      results:\n",
        "        An iterable of tuples. The first element is the cut_id, the second is\n",
        "        the reference transcript and the third element is the predicted result.\n",
        "      enable_log:\n",
        "        If True, also print detailed WER to the console.\n",
        "        Otherwise, it is written only to the given file.\n",
        "    Returns:\n",
        "      Return None.\n",
        "    \"\"\"\n",
        "    subs: Dict[Tuple[str, str], int] = defaultdict(int)\n",
        "    ins: Dict[str, int] = defaultdict(int)\n",
        "    dels: Dict[str, int] = defaultdict(int)\n",
        "\n",
        "    # `words` stores counts per word, as follows:\n",
        "    #   corr, ref_sub, hyp_sub, ins, dels\n",
        "    words: Dict[str, List[int]] = defaultdict(lambda: [0, 0, 0, 0, 0])\n",
        "    num_corr = 0\n",
        "    ERR = \"*\"\n",
        "\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR, sclite_mode=False)\n",
        "        for ref_word, hyp_word in ali:\n",
        "            if ref_word == ERR:\n",
        "                ins[hyp_word] += 1\n",
        "                words[hyp_word][3] += 1\n",
        "            elif hyp_word == ERR:\n",
        "                dels[ref_word] += 1\n",
        "                words[ref_word][4] += 1\n",
        "            elif hyp_word != ref_word:\n",
        "                subs[(ref_word, hyp_word)] += 1\n",
        "                words[ref_word][1] += 1\n",
        "                words[hyp_word][2] += 1\n",
        "            else:\n",
        "                words[ref_word][0] += 1\n",
        "                num_corr += 1\n",
        "    ref_len = sum([len(r) for _, r, _ in results])\n",
        "    sub_errs = sum(subs.values())\n",
        "    ins_errs = sum(ins.values())\n",
        "    del_errs = sum(dels.values())\n",
        "    tot_errs = sub_errs + ins_errs + del_errs\n",
        "    tot_err_rate = \"%.2f\" % (100.0 * tot_errs / ref_len)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"%WER {tot_errs / ref_len:.2%} \"\n",
        "        f\"[{tot_errs} / {ref_len}, {ins_errs} ins, \"\n",
        "        f\"{del_errs} del, {sub_errs} sub ]\"\n",
        "    )\n",
        "\n",
        "    print(f\"%WER = {tot_err_rate}\")\n",
        "    print(\n",
        "        f\"Errors: {ins_errs} insertions, {del_errs} deletions, \"\n",
        "        f\"{sub_errs} substitutions, over {ref_len} reference \"\n",
        "        f\"words ({num_corr} correct)\",\n",
        "    )\n",
        "    print(\n",
        "        \"Search below for sections starting with PER-UTT DETAILS:, \"\n",
        "        \"SUBSTITUTIONS:, DELETIONS:, INSERTIONS:, PER-WORD STATS:\",\n",
        "    )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-UTT DETAILS: corr or (ref->hyp)  \")\n",
        "    for cut_id, ref, hyp in results:\n",
        "        ali = kaldialign.align(ref, hyp, ERR)\n",
        "        combine_successive_errors = True\n",
        "        if combine_successive_errors:\n",
        "            ali = [[[x], [y]] for x, y in ali]\n",
        "            for i in range(len(ali) - 1):\n",
        "                if ali[i][0] != ali[i][1] and ali[i + 1][0] != ali[i + 1][1]:\n",
        "                    ali[i + 1][0] = ali[i][0] + ali[i + 1][0]\n",
        "                    ali[i + 1][1] = ali[i][1] + ali[i + 1][1]\n",
        "                    ali[i] = [[], []]\n",
        "            ali = [\n",
        "                [\n",
        "                    list(filter(lambda a: a != ERR, x)),\n",
        "                    list(filter(lambda a: a != ERR, y)),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "            ali = list(filter(lambda x: x != [[], []], ali))\n",
        "            ali = [\n",
        "                [\n",
        "                    ERR if x == [] else \" \".join(x),\n",
        "                    ERR if y == [] else \" \".join(y),\n",
        "                ]\n",
        "                for x, y in ali\n",
        "            ]\n",
        "\n",
        "        print(\n",
        "            f\"{cut_id}:\\t\"\n",
        "            + \" \".join(\n",
        "                (\n",
        "                    ref_word if ref_word == hyp_word else f\"({ref_word}->{hyp_word})\"\n",
        "                    for ref_word, hyp_word in ali\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"SUBSTITUTIONS: count ref -> hyp\")\n",
        "\n",
        "    for count, (ref, hyp) in sorted([(v, k) for k, v in subs.items()], reverse=True):\n",
        "        print(f\"{count}   {ref} -> {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"DELETIONS: count ref\")\n",
        "    for count, ref in sorted([(v, k) for k, v in dels.items()], reverse=True):\n",
        "        print(f\"{count}   {ref}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"INSERTIONS: count hyp\")\n",
        "    for count, hyp in sorted([(v, k) for k, v in ins.items()], reverse=True):\n",
        "        print(f\"{count}   {hyp}\")\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"PER-WORD STATS: word  corr tot_errs count_in_ref count_in_hyp\")\n",
        "    for _, word, counts in sorted(\n",
        "        [(sum(v[1:]), k, v) for k, v in words.items()], reverse=True\n",
        "    ):\n",
        "        (corr, ref_sub, hyp_sub, ins, dels) = counts\n",
        "        tot_errs = ref_sub + hyp_sub + ins + dels\n",
        "        ref_count = corr + ref_sub + dels\n",
        "        hyp_count = corr + hyp_sub + ins\n",
        "\n",
        "        print(f\"{word}   {corr} {tot_errs} {ref_count} {hyp_count}\")\n",
        "    return float(tot_err_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWlAHt34ktNF"
      },
      "source": [
        "Run the above code. Check the most common errors (insertion, substituion, deletion) and propose some methods to fix the issue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0EirLAkZS2VW"
      },
      "outputs": [],
      "source": [
        "get_error_stats(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "get_error_stats(results_beam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:** Obersvations and Proposed Methods to Fix Decoding Issues\n",
        "\n",
        "**1. Observations**\n",
        "1. **Default Decoding: greedy Search**:\n",
        "   - Lower Word Error Rate (WER: 100.00%) but with notable **substitutions (872)**, **deletions (127)**, and **insertions (28)**\n",
        "   - Fewer insertion and substitution errors than beam search.\n",
        "   - Higher deletion errors compared to beam search.\n",
        "   - **Common Errors:**\n",
        "      - **Substitution Example:** `(YEAH → Thank you)`  \n",
        "      - **Deletion Example:** `(YOU → ∅)`  \n",
        "      - **Insertion Example:** `(∅ → you.)`  \n",
        "2. **Beam Search**:\n",
        "   - Slightly higher WER (\\~101.56%).\n",
        "   - Fewer deletions but increased insertions and substitutions.\n",
        "   - Slight improvement in word inclusion (one additional correct word).\n",
        "\n",
        "\n",
        "**2. Proposed Fixes**\n",
        "\n",
        "- Reduce Substitution Errors:\n",
        "   - **Observation:** Many substitutions occur with casual terms like *\"Yeah,\"* *\"You,\"* and *\"Okay.\"*\n",
        "\n",
        "   - **Fine-tune Language Model on Conversational Data**\n",
        "   - *Example Fix:* `(YEAH → Yeah)` instead of `(YEAH → Thank you)`\n",
        "   - one could train the model specifically on AMI meeting datasets.\n",
        "\n",
        "   - **Context-Aware Decoding**\n",
        "   - *Example Fix:* `(OH SHE HAS → Oh, she has)` instead of `(OH SHE HAS → Oh, shit)`\n",
        "   - so use a context-aware decoder to consider surrounding sentence structure.\n",
        "\n",
        "- Reduce Deletion Errors:\n",
        "   - **Observation:** Frequent deletions of shorter words (*\"you\"*, *\"it\"*, *\"well\"*) impact comprehension.\n",
        "\n",
        "   - **Adjust Confidence Thresholds**\n",
        "   - *Example Fix:* `(YOU → you)` instead of omitting \"YOU.\"\n",
        "   - so one could  retain low-confidence predictions if they are likely to be meaningful.\n",
        "\n",
        "Reduce Insertion Errors:\n",
        "   - **Observation:** Insertions like `you.` or `something.` degrade sentence clarity.\n",
        "\n",
        "   - **Adjust Beam Size**\n",
        "   - *Example Fix:* `(∅ → you.)` avoided by using a smaller beam size.\n",
        "   - one could experiment with beam sizes (e.g., reduce from 4 to 3/2).\n",
        "\n",
        "- **Length Penalty in Decoding**\n",
        "   - *Example Fix:* `(OH → No)` instead of generating unnecessary words.\n",
        "   - one could also maybe apply penalties for overly verbose outputs.\n",
        "\n",
        "Post-Processing with Heuristics:\n",
        "   - **Rule-Based Post-Processing**\n",
        "   - *Example Fix:* Remove repetitive outputs like `(MM → *)`.\n",
        "   - by maybe applying regex-based rules to clean outputs.\n",
        "\n",
        "   - **Error Correction Module**\n",
        "   - *Example Fix:* Replace hallucinated outputs like `(SNORE → Star)` with more plausible predictions.\n",
        "   - one could use an external correction layer trained on common AMI transcription errors.\n",
        "\n",
        "   - **Fine-Tuning**:\n",
        "   - one could also try to fine-tune the model on AMI meeting data to improve performance on this specific domain( meetings).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfxSZskIk0RM"
      },
      "source": [
        "### Text normalization (20 pt)\n",
        "\n",
        "Whisper outputs human readable text by default. Notice that the this is not the case for reference.\n",
        "\n",
        "Normalize the reference and/or hypothesis text output in a way that both of them match. Then compute the WER again with ``get_error_stats()``.\n",
        "\n",
        "You may Google for any Python-relevant help for string processing. If you use an external reference, please add the reference in the comment.\n",
        "\n",
        "You may reuse code from previous sections to compute WER.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "a4rTYKdhkzxC"
      },
      "outputs": [],
      "source": [
        "# Enter your code here. You can use multiple cells if necessary.\n",
        "# Code \"may\" be longer than 3 statements in this case.\n",
        "import spacy # using spaCy library\n",
        "from cleantext import clean # using clean-text library\n",
        "from jiwer import wer\n",
        "import re\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "replacement_dict = {\n",
        "    \"uh\": \"\", \"um\": \"\", \"mm-hmm\": \"yes\", \"mm\": \"\", \"hmm\": \"\", \"'cause\": \"because\",\n",
        "    \"yeah\": \"yes\", \"okay\": \"ok\", \"okey\": \"ok\", \"uh-huh\": \"yes\", \"uh-huh.\": \"yes\",\n",
        "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"kinda\": \"kind of\", \"lemme\": \"let me\",\n",
        "    \"gotta\": \"got to\", \"ain't\": \"is not\", \"y'all\": \"you all\", \"n't\": \" not\",\n",
        "    \"it's\": \"it is\", \"i'm\": \"i am\", \"you're\": \"you are\", \"they're\": \"they are\",\n",
        "    }\n",
        "\n",
        "def replace_with_dict(text, replacements):\n",
        "    for key, value in replacements.items():\n",
        "        text = re.sub(rf'\\b{key}\\b', value, text, flags=re.IGNORECASE)\n",
        "    return text\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Normalize text using spaCy and clean-text.\n",
        "    \"\"\"\n",
        "    #Clean text - clean-text libary\n",
        "    text = clean(text,\n",
        "        lower=True,\n",
        "        no_punct=True,\n",
        "        no_numbers=False,\n",
        "        replace_with_punct=\"\",\n",
        "        replace_with_url=\"\",\n",
        "        replace_with_email=\"\",\n",
        "        replace_with_number=\"\",\n",
        "        lang=\"en\")\n",
        "    # Step 2: Apply custom replacements\n",
        "    text = replace_with_dict(text, replacement_dict)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    #tokenize/process - spaCy\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_punct]\n",
        "    normalized_text = \" \".join(tokens).strip()\n",
        "    return normalized_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "y5ijJw1yVkIJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('EN2002b-1-170-ihm', 'goals or something', 'goals or something')"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "results = []\n",
        "for utt in utt2ref:\n",
        "    if utt not in utt2hyp:\n",
        "        print(f\"ERROR: Missing hypothesis for utt {utt}\")\n",
        "        continue\n",
        "    ref_tokens = normalize_text(utt2ref[utt])\n",
        "    hyp_tokens = normalize_text(utt2hyp[utt])\n",
        "    \n",
        "    results.append((utt, ref_tokens, hyp_tokens))\n",
        "\n",
        "results[0] # example showing what results contains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DUH6cCaEVwtj"
      },
      "outputs": [],
      "source": [
        "get_error_stats(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLhaHy7GqOpM"
      },
      "source": [
        "## 2.5 Error analysis (5 pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToSQOh2dXmkL"
      },
      "source": [
        "Pick two specific types of errors and discuss how we can improve the ASR for such errors. Note that we are not referring to INSERTION, DELETION and SUBSTITUTION errors, but something more specific. For instance, you could mention a particular error like 'TWO -> TO' and propose a remedy. Choose the ASR output with the best WER so far for your analysis.\n",
        "\n",
        "---\n",
        "Answer:\n",
        "With a WER of 17.67%, the ASR system has shown significant improvement. However, certain error patterns still persist. Two specific types of errors that can be addressed are:\n",
        "\n",
        "**Obersvation:**\n",
        "- Example error 1 : EN2002b-0-36-ihm:\t(h e->i t) ... (so maybe: reference: he does not have to know exactly, hypothesis: it does not have to know exactly)\n",
        "- Example error 2 : EN2002b-3-2-ihm: (c->t) h a n (n e l->k y o u) ...  ( so maybe: reference: channel you, hypothesis: thank you)\n",
        "- Example error 3 : EN2002b-3-168-ihm: ... (*->b e) c a u s e i m s u r e ...( referece could be: cause I'm sure you've probably made some changes already, hypothesis: because I'm sure you've probably made some changes already)\n",
        "- Example error 4: EN2002b-3-14-ihm: (o h s h (e h a s → i t)) ... (so maybe: reference: oh she has, hypothesis: oh it)\n",
        "\n",
        "**Disscussion on Imporvement and Causes:**\n",
        "- In example 4, the model struggles with acoustic ambiguity. Fillers like \"oh\", \"um\", and \"uh\" have low phonetic energy and are often misinterpreted as content words.\n",
        "Instead of deprioritizing fillers, the language model (LM) interprets them as noise rather than meaningful words. To impove this one could introduce explicit filler tokens in the LM training data to help the model distinguish between fillers and content words. one could also try to use a disfluency detection module to identify and filter out fillers before decoding.\n",
        "- in exaple error 1 , the pronoun \"he\" is replaced with \"it\" due to contextual ambiguity and phonetic similarity.\n",
        "Similarly, in Example 3, the substitution \"cause\" becomes \"because\"—likely due to LM bias towards more frequent grammatical structures. To address this one could use a context-aware language model (BERT/GPT) that can better capture pronoun dependencies and sentence structure. one could also try to use a speaker-aware LM to adapt to individual speaking styles and improve pronoun resolution.\n",
        "- In example 2, the substitution of \"channel\" with \"thank\" highlights phonetic overlap. Short, low-energy words with similar phonetic patterns are prone to errors especially in spontaneous speech. One could address this by doing a phoneme-aware fine-tuning to better handle phonetic similar words or could trian ASR system on spontaneous speech dataset where similar overlaps frequently occur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHmJ23-tkkUJ"
      },
      "source": [
        "### 2.6 Plot WER vs utterance duration (10pt)\n",
        "\n",
        "Create a plot to check if the duration of the utterance is related to the WER. Conclude upon visual observation if there exists a correlation between the two quantities. Use the system with the best WER so far. Justify your response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation Coefficient: -0.17643302666339505\n"
          ]
        }
      ],
      "source": [
        "# Convert to DataFrame\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import jiwer\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"utterance\", \"ref\", \"hyp\"])\n",
        "\n",
        "df = df[(df['ref'] != '') & (df['hyp'] != '')] # remove empty utterances (if any)\n",
        "df['WER'] = df.apply(lambda row: jiwer.wer(row['ref'], row['hyp']), axis=1)\n",
        "df['duration'] = df['ref'].apply(lambda x: len(x.split()) * 0.5) # approx. utterance duration (0.5s per word in reference)\n",
        "\n",
        "#df.head()\n",
        "#plot \n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.scatterplot(data=df, x='duration', y='WER', s=100, color='b', alpha=0.7)\n",
        "plt.title('WER vs Utterance Duration')\n",
        "plt.xlabel('Utterance Duration (seconds)')\n",
        "plt.ylabel('Word Error Rate (WER)')\n",
        "plt.show()\n",
        "\n",
        "correlation = df['duration'].corr(df['WER'])\n",
        "print(f\"Correlation Coefficient: {correlation}\") # weak correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observation: \n",
        "From the visual observation, there is no clear positive correlation between utterance duration and WER. However:\n",
        "- Short utterances show higher variability in WER, with occasional extreme outliers.\n",
        "- Longer utterances tend to have lower and more consistent WER values, possibly because they provide more acoustic context for the ASR system to perform accurate predictions.\n",
        "\n",
        "These findings suggest that utterance length alone does not strongly dictate WER, but shorter utterances are more prone to variability in recognition performance, likely due to insufficient acoustic context or ambiguity in phonetic content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzA7rKZi4wr4"
      },
      "source": [
        "## 3. Whisper tokenizer (15pt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSY5OEZeLmC4"
      },
      "source": [
        "In the next code cell, the tokenizer used by multilingual models is initialized in the variable ``tokenizer``. Without changing the ``tokenizer`` variable, do the following:\n",
        "\n",
        "1. Print the token index of beginning of sentence and end of sentence special tokens\n",
        "2. Based on [this dictionary](https://github.com/openai/whisper/blob/90db0de1896c23cbfaf0c58bc2d30665f709f170/whisper/tokenizer.py#L10), print the the token index of two languages: French and German. An example is already given below for English.\n",
        "3. Given that the supported languages are in ``whisper.tokenizer.LANGUAGES``, find out only using python code if (a) your native tongue is supported (b) the language Kurmanji is supported."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "xiq1IRCR3V4p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:29<00:00, 4.85MiB/s]\n"
          ]
        }
      ],
      "source": [
        "from whisper import load_model\n",
        "\n",
        "model = load_model('base', device='cpu', download_root='./')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "T56NbrBr4wXS"
      },
      "outputs": [],
      "source": [
        "import whisper.tokenizer as whisper_tokenizer\n",
        "\n",
        "tokenizer = whisper_tokenizer.get_tokenizer(\n",
        "    True,  # assume a multilingual model\n",
        "    num_languages=model.num_languages,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "sG_66a2Q30WH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Special Tokens:\n",
            "Token index for Beginning of Sentence (BOS): 50258\n",
            "Token index for End of Sentence (EOS): 50257\n"
          ]
        }
      ],
      "source": [
        "## 1. Print the token index of bos, eos (2.5 pt)\n",
        "bos_token_index = tokenizer.special_tokens.get('<|startoftranscript|>')\n",
        "eos_token_index = tokenizer.special_tokens.get('<|endoftext|>')\n",
        "\n",
        "print(\"1. Special Tokens:\")\n",
        "print(f\"Token index for Beginning of Sentence (BOS): {bos_token_index}\")\n",
        "print(f\"Token index for End of Sentence (EOS): {eos_token_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "XtqMe7Do37qf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50259"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# This is an example to get the language token for English.\n",
        "tokenizer.special_tokens.get('<|en|>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "WbTGW92Z5gsL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "2. Language Tokens:\n",
            "Token index for French: 50265\n",
            "Token index for German: 50261\n"
          ]
        }
      ],
      "source": [
        "## 2. print the the token index of French and German (2.5 pt)\n",
        "french_token_index = tokenizer.special_tokens.get('<|fr|>')\n",
        "german_token_index = tokenizer.special_tokens.get('<|de|>')\n",
        "\n",
        "print(\"\\n2. Language Tokens:\")\n",
        "print(f\"Token index for French: {french_token_index}\")\n",
        "print(f\"Token index for German: {german_token_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "S4k5jTHa5rQc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Language Support Check:\n",
            "Is SwissGerman tongue ('swiss') supported? No\n",
            "Is Swedish tongue ('sv') supported? Yes\n",
            "Is Kurmanji ('ku') supported? No\n"
          ]
        }
      ],
      "source": [
        "## 3. Find out only using python code if (a) your native tongue is supported (b) the language Kurmanji is supported. (10 pt)\n",
        "print(\"\\n3. Language Support Check:\")\n",
        "# a) Check if Swedish is supported : Checked first if SwissGerman but that is not supported\n",
        "native_language = 'swiss' \n",
        "is_native_supported = native_language in whisper_tokenizer.LANGUAGES\n",
        "print(f\"Is SwissGerman tongue ('{native_language}') supported? {'Yes' if is_native_supported else 'No'}\")\n",
        "native_language = 'sv' # SwissGerman is not supported  swedish is\n",
        "is_native_supported = native_language in whisper_tokenizer.LANGUAGES\n",
        "print(f\"Is Swedish tongue ('{native_language}') supported? {'Yes' if is_native_supported else 'No'}\")\n",
        "\n",
        "# b) Check if Kurmanji is supported\n",
        "is_kurmanji_supported = 'ku' in whisper_tokenizer.LANGUAGES  # 'ku' is the ISO 639-1 code for Kurdish\n",
        "print(f\"Is Kurmanji ('ku') supported? {'Yes' if is_kurmanji_supported else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su9OeUZ0VzpK"
      },
      "source": [
        "## 1D convolution vs TDNN (25pt)\n",
        "\n",
        "As seen in class, Whisper uses two convolutional layers before the Transformer layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "OUehV40XV7HN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "n_mels = 80\n",
        "n_state = 768\n",
        "conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3)\n",
        "conv2 = nn.Conv1d(n_state, n_state, kernel_size=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "a7RCH-i4eg9d"
      },
      "outputs": [],
      "source": [
        "# 1. Create an appropriate input for the above convolutional layer, and apply it\n",
        "# to the first convolutional layer. The input is a 3-dimensional tensor.\n",
        "# The first dimenion is a number of sequences, we assume there is only one sequence.\n",
        "# The second dimenion is the number of filterbank energies per frame.\n",
        "# The third dimension is the maximum audio length in number of frames.\n",
        "\n",
        "x = torch.randn(1, n_mels, 3000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "cEt-O6dfhH_0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 768, 2996])"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 2. Get the output after passing it through the first 2 convolution layers.\n",
        "\n",
        "# forward pass\n",
        "x = F.gelu(conv1(x))\n",
        "x = F.gelu(conv2(x))\n",
        "\n",
        "# debug statement only, not part of forward pass\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "pkfovmrhiBPf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final output shape: torch.Size([1, 768, 1000])\n"
          ]
        }
      ],
      "source": [
        "# 3. Adjust the hyperparameters of the two Conv1d layers above to get an output\n",
        "# of sequence length 1000. Please re-write all code in this cell here. Do not\n",
        "# modify the code above.\n",
        "\n",
        "# conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, bias=False, padding=1)\n",
        "# conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, padding=1, stride=3)\n",
        "n_mels = 80\n",
        "n_state = 768\n",
        "conv1 = nn.Conv1d(n_mels, n_state, kernel_size=3, stride=2, padding=1, bias=False)  # Halve length\n",
        "conv2 = nn.Conv1d(n_state, n_state, kernel_size=3, stride=1, padding=1)  # Preserve length after interpolation\n",
        "\n",
        "x = torch.randn(1, n_mels, 3000)\n",
        "# Forward pass\n",
        "x = F.gelu(conv1(x))  #first convolution (1500 length)\n",
        "x = F.interpolate(x, size=1000, mode='linear')  #resize to 1000 length\n",
        "x = F.gelu(conv2(x))  #preserving length after interpolation\n",
        "print(\"final output shape:\", x.shape)\n",
        "\n",
        "# Verify the output length\n",
        "assert x.shape == torch.Size([1, n_state, 1000]), \"Output sequence length is not 1000\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQD-eQygpuME"
      },
      "source": [
        "4. Would your approach to getting the desired sequence length at the output of the two convolutional layers change if we did not apply the non-linearity in the forward pass?\n",
        "\n",
        "**Answer:**\n",
        "No, if non-linearity (GELU) were removed, the sequence length would not change because non-linearity does not affect the spatial dimensions of the output tensor. Convolution operations (stride, padding, kernel_size) primarily dictate the sequence length.However, removing non-linearity might affect the model's ability to learn complex patterns since non-linear transformations add expressiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKzZmNCqjdNe"
      },
      "source": [
        "5. Use the Conv1D class instead of the TDNN class in Exercise 4 to create one of the layers of the X-vector network. The details about which layer is given in the code below.\n",
        "\n",
        "NOTE: Only re-write it. No need to test it (unless you prefer to demonstrate that they are equal)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "9fw7QlHcj0Oe"
      },
      "outputs": [],
      "source": [
        "# The class is commented. It is only to be used as a reference\n",
        "class TDNN(nn.Module):\n",
        "     def __init__(\n",
        "         self,\n",
        "         feat_dim,\n",
        "         output_dim,\n",
        "         context_len=1,\n",
        "     ):\n",
        "         super(TDNN, self).__init__()\n",
        "         self.linear = nn.Linear(feat_dim*context_len, output_dim)\n",
        "         self.context_len = torch.tensor(context_len, requires_grad=False)\n",
        "\n",
        "\n",
        "     def forward(self, input):\n",
        "         mb, T, D = input.shape\n",
        "         padded_input = input.reshape(mb, -1).unfold(1, D*self.context_len, D).contiguous()\n",
        "         x = self.linear(padded_input)\n",
        "         return x\n",
        "\n",
        "# Rewrite the line below with Conv1D\n",
        "tdnn1 = TDNN(feat_dim=40, output_dim=128, context_len=3)\n",
        "\n",
        "#tdnn1 = nn.Conv1D(...)\n",
        "conv1d_layer = nn.Conv1d(\n",
        "    in_channels=40,   \n",
        "    out_channels=128,  \n",
        "    kernel_size=3,     #context_len\n",
        "    stride=1,          #match TDNN step size\n",
        "    padding=0,  \n",
        "    bias=False          \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TDNN Output Shape: torch.Size([1, 98, 128])\n",
            "Conv1D Output Shape: torch.Size([1, 98, 128])\n"
          ]
        }
      ],
      "source": [
        "# dummy input\n",
        "x = torch.randn(1, 100, 40) \n",
        "tdnn_output = tdnn1(x)\n",
        "print(\"TDNN Output Shape:\", tdnn_output.shape)\n",
        "\n",
        "# Conv1D layer forward pass\n",
        "x_conv = x.permute(0, 2, 1) \n",
        "conv_output = conv1d_layer(x_conv)\n",
        "conv_output = conv_output.permute(0, 2, 1)\n",
        "print(\"Conv1D Output Shape:\", conv_output.shape)\n",
        "\n",
        "assert tdnn_output.shape == conv_output.shape, \"mismatch in output shapes!\"\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "GPU-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
